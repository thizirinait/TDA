{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d8204a",
   "metadata": {},
   "source": [
    "# TDA: TP 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a559527",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Statistics, Unsupervised, and Supervised  Machine Learning on 3D shapes with Topological Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23287a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this practical session, we will use the various TDA tools presented in class in order to run data science tasks (inference, clustering, classification) on a data set of 3D shapes. As in the first practical session, we will use  [`Gudhi`](https://gudhi.inria.fr/) (see first practical session for installation instructions). The different sections of this notebook can be run independently (except Section 0 which is mandatory), so feel free to start with the project that sounds the more interesting to you :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35878d1d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note also that if you choose to switch from a section to another, make sure to clear all variables first (and run Section 0 again) since some variable names are shared between sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467250e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gudhi as gd\n",
    "print(gd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea233b32",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gudhi.clustering.tomato as gdt\n",
    "import gudhi.representations   as gdr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8e9ae",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `TensorFlow` module of `Gudhi` is only required in Section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09350bb7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gudhi.tensorflow as gdtf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ab5d4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Other than that, you are free to use whatever other Python package you feel comfortable with :-) We make some suggestions below (these dependencies are also required to run our solutions to the exercises). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf83d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37365ef4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use three standard Python libraries: `NumPy`, `Scipy` and `Matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00458c7c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5bd90",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2684f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to visualize 3D shapes, we will use [`meshplot`](https://skoch9.github.io/meshplot/tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452760a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import meshplot as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05b39a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, some dependencies are section-specific: we list those below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c29ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Section 1, when running bootstrap tests on ToMATo, we will use the `statistics` Python module. These tests will be based on the Laplace-Beltrami operator, which can be computed with [`robust_laplacian`](https://pypi.org/project/robust-laplacian/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f816e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "import robust_laplacian as rlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28af1f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Section 2, we will use the [`networkx`](https://networkx.org/) package to visualize and run computations on Mapper graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f20505",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0d9fb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Sections 3 and 4, when computing vectorizations and performing supervised machine learning and deep learning tasks, we will use various modules of [`Scikit-Learn`](https://scikit-learn.org/stable/index.html) and [`TensorFlow`](https://www.tensorflow.org/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b4948",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing   as skp\n",
    "import sklearn.neighbors       as skn\n",
    "import sklearn.model_selection as skm\n",
    "import sklearn.decomposition   as skd\n",
    "import sklearn.manifold        as skf\n",
    "import sklearn.pipeline        as skl\n",
    "import sklearn.svm             as sks\n",
    "import sklearn.ensemble        as ske"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893f1a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da235d11",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section 0: Data set manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fe303",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are good to go! First things first, we have to download the data set. It can be obtained [here](https://people.cs.umass.edu/~kalo/papers/LabelMeshes/labeledDb.7z). Extract it, and save its path in the `dataset_path` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba968f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_path = './3dshapes/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d7c06",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see, the data set in split in several categories (`Airplane`, `Human`, `Teddy`, etc), each category having its own folder. Inside each folder, some 3D shapes (i.e., 3D triangulations) are provided in [`.off`](https://en.wikipedia.org/wiki/OFF_(file_format)) format, and face (i.e., triangle) labels are provided in text files (extension `.txt`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601b34b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Every data science project begins by some preprocessing ;-) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac673d5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write a function `off2numpy` that reads information from an `.off` file and store it in two `NumPy` arrays, called `vertices` (type float and shape number_of_vertices x 3---the 3D coordinates of the vertices) and `faces` (type integer and shape number_of_faces x 3---the IDs of the vertices that create faces). Write also a function `get_labels` that stores the face labels of a given 3D shape in a `NumPy` array (type string or integer and shape [number_of_faces].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15b400",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def off2numpy(shape_name):\n",
    "    with open(shape_name, 'r') as S:\n",
    "        S.readline()\n",
    "        num_vertices, num_faces, _ = [int(n) for n in S.readline().split(' ')]\n",
    "        info = S.readlines()\n",
    "    vertices = np.array([[float(coord) for coord in l.split(' ')] for l in info[0:num_vertices]])\n",
    "    faces    = np.array([[int(coord) for coord in l.split(' ')[1:]] for l in info[num_vertices:]])\n",
    "    return vertices, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f918639",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_labels(label_name, num_faces):\n",
    "    L = np.empty([num_faces], dtype='|S100')\n",
    "    with open(label_name, 'r') as S:\n",
    "        info = S.readlines()\n",
    "    labels, face_indices = info[0::2], info[1::2]\n",
    "    for ilab, lab in enumerate(labels):\n",
    "        indices = [int(f)-1 for f in face_indices[ilab].split(' ')[:-1]]\n",
    "        L[  np.array(indices)  ] = lab[:-1]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afeb43",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can now apply your code and use `meshplot` to visualize a given 3D shape, say `61.off` in `Airplane`, and the labels on its faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778d918",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vertices, faces = off2numpy(dataset_path + 'Airplane/61.off')\n",
    "label_faces = get_labels(dataset_path + 'Airplane/61_labels.txt', len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0eea2d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mp.plot(vertices, faces, c=skp.LabelEncoder().fit_transform(label_faces))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1639d038",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section 1: 3D robust segmentation with ToMATo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7a831",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, our goal is to use the ToMATo algorithm to compute segmentations of 3D shapes, i.e., to assign labels to 3D shape vertices in an unsupervised way, that is, without training on known labels. This task was initially explored in [this article](https://www.lix.polytechnique.fr/~maks/papers/pers_seg.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b467f5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Overall, the main idea is to run ToMATo on the neighborhood graph given by the triangulation, with the so-called Heat Kernel Signature (HKS) as the filter. This is motivated by the fact that the HKS function typically takes higher values on the parts of the 3D shape that are very curved (such as, e.g., the tips of fingers in human hand shapes).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850954e9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The HKS was defined in [this article](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1467-8659.2009.01515.x). It is related to the heat equation on a given 3D shape $S$:\n",
    "\n",
    "$$\\Delta_S f = -\\frac{\\partial f}{\\partial t}.$$\n",
    "\n",
    "More formally, the HKS function  with parameter $t >0$ on a vertex $v\\in S$, and denoted by ${\\rm HKS}_t(v)$, is computed as:\n",
    "\n",
    "$${\\rm HKS}_t(v) = \\sum_{i=0}^{+\\infty} {\\rm exp}(-\\lambda_i\\cdot t)\\cdot \\phi_i^2(v),$$\n",
    "\n",
    "where $\\{\\lambda_i, \\phi_i\\}_i$ are the eigenvalues and eigenvectors of $\\Delta_S$.\n",
    "Intuitively, ${\\rm HKS}_t(v)$ is the amount of heat remaining on $v$ at time $t$, after unit sources of heat have been placed on each vertex at time `t=0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366cbe56",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's first pick a 3D shape. For instance, use `Hand/181.off` (or any other one you would like to try)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649d292",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vertices, faces = off2numpy(dataset_path + 'Hand/181.off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a2e3d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, use `robust_laplacian` to compute the first 200 eigenvalues and eigenvectors of its Laplacian (you can use the `eigsh` function of `SciPy` for diagonalizing the Laplacian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203c5dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "laplacian, mass = rlap.mesh_laplacian(vertices, faces)\n",
    "egvals, egvecs = sp.sparse.linalg.eigsh(laplacian, 200, mass, sigma=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b239584",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write a function `HKS` that uses these eigenvalues and eigenvectors, as well as a time parameter, to compute the HKS value on a given vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2003fcd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def HKS(t, egvals, evecs):\n",
    "    return np.sum(np.multiply( np.exp(-egvals * t)[None,:], np.square(egvecs) ), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c67cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualize the function values with `meshplot` for different time parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750730f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mp.plot(vertices, faces, c=HKS(1e-1, egvals, egvecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f1141",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Recall that ToMATo requires, in addition to the filter, a neighborhood graph built on top of the data. Fortunately, we can use the triangulations of our 3D shapes as input graphs! Write a function `get_neighborhood_graph_from_faces` that computes a neighborhood graph (in the format required by ToMATo) from the faces of a triangulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2476ac7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_neighborhood_graph_from_faces(faces, num_vertices):\n",
    "    NG = [[] for _ in range(num_vertices)]\n",
    "    for face in faces:\n",
    "        [i1, i2, i3] = face\n",
    "        NG[i1].append(i2)\n",
    "        NG[i2].append(i1)\n",
    "        NG[i2].append(i3)\n",
    "        NG[i3].append(i2)\n",
    "        NG[i1].append(i3)\n",
    "        NG[i3].append(i1)\n",
    "    NG = [np.unique(neighbs) for neighbs in NG]\n",
    "    return NG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b145ab0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, apply ToMATo (with no prior on the number of clusters or merging threshold) on the neighborhood graph and the HKS function associated to a given time parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487dd288",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neighborhood_graph = get_neighborhood_graph_from_faces(faces, len(vertices))\n",
    "function = HKS(1e-1, egvals, egvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bc695",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tomato = gdt.Tomato(graph_type='manual', density_type='manual', n_clusters=None, merge_threshold=0)\n",
    "tomato = tomato.fit(X=neighborhood_graph, weights=function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64dcf8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualize the persistence diagram produced by ToMATo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7448b0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tomato.plot_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabf76b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many points do you see standing out from the diagonal? Use this number to re-cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a35082",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tomato.n_clusters_ = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2ef7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualize the 3D shape with the ToMATo labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793442bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mp.plot(vertices, faces, c=tomato.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74357858",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Does our segmentation make sense? Can you interpret the boundaries between labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bcd7df",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since the boundaries are driven by the elder rule, they can seem a bit shaggy. In order to fix this, we can use bootstrap-like smoothing. The idea is to first save the current ToMATo clustering obtained with filter $f$ (let's call it the initial clustering), and then perturb $f$ a little bit into another function $\\tilde f$, and finally recompute clustering with ToMATo using $\\tilde f$. Since clusters are now created with the maxima of $\\tilde f$ (which will be different in general from those of $f$), we can use the initial clustering to relate the clusters of $\\tilde f$ to those of $f$, by simply looking at which (initial) clusters do the maxima of $\\tilde f$ belong to. If we repeat this procedure $N$ times, we will end up with a distribution (of size $N$) of candidate clusters for each vertex $v$. It suffices to pick the most frequent one for each vertex to get a smooth segmentation for the 3D shape. See also Section 6 in [the article](https://www.lix.polytechnique.fr/~maks/papers/pers_seg.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada5419",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to implement this, write first a function `get_indices_of_maxima` which computes the indices of the maxima  associated to a set of ToMATo clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea88d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_indices_of_maxima(label_points, function):\n",
    "    Li = np.copy(label_points)\n",
    "    for lab in np.unique(label_points):\n",
    "        inds = np.argwhere(label_points == lab).ravel()\n",
    "        imax = np.argmax(function[inds]).ravel()\n",
    "        Li[inds] = inds[imax]\n",
    "    return Li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ffa0e8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute and plot these maxima on the 3D shape to make sure your code is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1f5a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tomato_maxima = get_indices_of_maxima(tomato.labels_, function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46848d92",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "unique_tomato_maxima = np.unique(tomato_maxima)\n",
    "vertex_show = np.concatenate([unique_tomato_maxima] + [neighborhood_graph[m] for m in unique_tomato_maxima])\n",
    "mp.plot(vertices, faces, c=np.array([1 if i in vertex_show else 0 for i in range(len(vertices))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb18cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, use this function to write another function `bootstrap_tomato` that perform a bootstrap smoothing of a set to ToMATo labels. This function will also take as arguments a number $N$ of bootstrap iterations, and a parameter $\\epsilon$ controlling the amplitude of the uniform noise used to perturb the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5f6fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_tomato(tomato_maxima, epsilon, N=100, num_labels=None):\n",
    "    distribution_maxima = np.zeros(shape=[len(tomato_maxima), N])\n",
    "    for n in range(N):\n",
    "        if (n+1) % 100 == 0:\n",
    "            print(str(n+1) + '/' + str(N))\n",
    "        np.random.seed(n)\n",
    "        noisy_function = function + np.random.uniform(low=-epsilon, high=epsilon, size=function.shape)\n",
    "        tomato_boot = gdt.Tomato(graph_type='manual', density_type='manual', n_clusters=num_labels)\n",
    "        tomato_boot.fit(X=neighborhood_graph, weights=noisy_function)\n",
    "        maxima_boot = get_indices_of_maxima(tomato_boot.labels_, noisy_function)\n",
    "        distribution_maxima[:,n] = tomato_maxima[maxima_boot]\n",
    "    final_labels = np.array([int(statistics.mode(distribution_maxima[i,:])) for i in range(len(tomato_maxima))])\n",
    "    return skp.LabelEncoder().fit_transform(final_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df1279",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Apply the bootstrap smoothing and visualize the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a5e01",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_labels = bootstrap_tomato(tomato_maxima, epsilon=12, N=300, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381921ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mp.plot(vertices, faces, c=final_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef2242",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Is the segmentation any better? How does the result depend on the noise amplitude?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9d012",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section 2: 3D shape skeletonization with Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855162df",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, our goal is to use Mapper to produce 1-skeletons (i.e., graphs) of 3D shapes. We will also see how to partition this graph into different parts and run statistical tests to decide whether these parts should be considered as numerical artifacts or true signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10707dfe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's first pick a 3D shape. For instance, use `Human/3.off` (or any other one you would like to try)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e3626",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vertices, faces = off2numpy(dataset_path + 'Human/4.off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e1a7b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mp.plot(vertices, faces, c=vertices[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f9c5b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In `Gudhi`, Mapper is implemented as a specific case of Graph Induced Complex (GIC), see Definition 2.1 in [this article](https://web.cse.ohio-state.edu/~dey.8/paper/GIC/GIC.pdf). Indeed, given a fixed vertex cover, Mapper computed with hierarchical clustering with parameter $\\delta > 0$ is (roughly) the same as GIC computed with neighborhood graph with parameter $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c15a60",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Initiate a `CoverComplex` from `Gudhi`, and set its type to `\"GIC\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a7e51",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex = gd.CoverComplex()\n",
    "mapper_complex.set_verbose(True)\n",
    "mapper_complex.set_type('GIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73023a59",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define the point cloud on which Mapper is computed with the array `vertices`, and the filter function as the height coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecb174",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex.set_point_cloud_from_range(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ce32d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex.set_function_from_coordinate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885038d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define the node color function (used only for visualization) as the height coordinate as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869fa9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex.set_color_from_coordinate(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd84f77",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define the clustering algorithm by automatically tuning the $\\delta$ parameter. This can be done by setting the neighborhood graph automatically with the `set_graph_from_automatic_rips` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d00ce7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimal_delta = mapper_complex.set_graph_from_automatic_rips()\n",
    "print('Optimal delta = ' + str(optimal_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a4f5b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, define the cover parameters: 20 intervals for the range of the filter (this parameter is called resolution), and 30% overlap (this one is called gain). Then, compute the cover using preimages of the intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc01401",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex.set_resolution_with_interval_number(20)\n",
    "mapper_complex.set_gain(0.3)\n",
    "mapper_complex.set_cover_from_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59772c4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can now compute Mapper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4377634",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex.find_simplices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4b64e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "During computations, the pairwise distances are saved in a binary file `matrix_dist` in order to save time for further computations. Hence, if you want to apply Mapper again on a different shape, make sure to remove this file!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa865be",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The simplicial complex produced by Mapper can be obtained with the `create_simplex_tree` function. However, its vertices are given integer IDs associated to the cover used to compute Mapper. For convenience, rename the vertices from 0 to number_of_vertices in increasing order of the initial IDs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b3f80",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "old_simplex_tree = mapper_complex.create_simplex_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce531d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_name = {k[0][0]: i for i,k in enumerate(old_simplex_tree.get_skeleton(0))}\n",
    "simplex_tree = gd.SimplexTree()\n",
    "for s,f in old_simplex_tree.get_simplices():\n",
    "    simplex_tree.insert([inv_name[v] for v in s],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe496a9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Gudhi` also computes the mean of the midpoint of the interval associated to each Mapper vertex, and store it as a filtration value. Check that you have correct filtration values in your simplex tree (at least by eye ;-))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba900dc2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Mapper is of dimension ' + str(simplex_tree.dimension()) + ' - ' + \\\n",
    "                                  str(simplex_tree.num_simplices()) + ' simplices - ' + \\\n",
    "                                  str(simplex_tree.num_vertices()) + ' vertices.')\n",
    "for simplex, filtration in simplex_tree.get_simplices():\n",
    "    print(simplex, filtration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60ac7d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With the `write_info` function, Gudhi can produce a `.txt` file containing information about the 1-skeleton of the Mapper, that can be processed by an utility function, available [here](https://github.com/GUDHI/gudhi-devel/blob/master/src/Nerve_GIC/utilities/KeplerMapperVisuFromTxtFile.py). Download and apply this utility function. This will produce an `.html` file that you can visualize in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f3b30",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex.write_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0a1a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!python ../../Git/gudhi/src/Nerve_GIC/utilities/KeplerMapperVisuFromTxtFile.py -f matrix_sc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2b3de",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another (more convenient) way to visualize our complex is to plot its 1-skeleton in a Python figure with `networkx`. Using `networkx` documentation, write a function `get_networkx` that turns a simplicial complex into a `networkx` graph corresponding to the 1-skeleton. Make it so the `networkx` graph has two attributes, `\"color\"` and `\"size\"` that contain the mean of the filter values of the points associated to the Mapper vertices, and the number of these points (i.e., the size of the preimages) respectively. For this, you can use `subpopulation` method of Mapper, which returns the point IDs corresponding to every Mapper vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5c4ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_networkx(mapper_complex, simplex_tree, get_attrs=False):\n",
    "    G = nx.Graph()\n",
    "    for (splx,_) in simplex_tree.get_skeleton(1):\n",
    "        if len(splx) == 1:\n",
    "            G.add_node(splx[0])\n",
    "        if len(splx) == 2:\n",
    "            G.add_edge(splx[0], splx[1])\n",
    "    if get_attrs:\n",
    "        attrs = {k: vertices[np.array(mapper_complex.subpopulation(k)),1].mean() for k in G.nodes()}\n",
    "        nx.set_node_attributes(G, attrs, name='color')\n",
    "        attrs = {k: len(mapper_complex.subpopulation(k)) for k in G.nodes()}\n",
    "        nx.set_node_attributes(G, attrs, name='size')\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194959ce",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Apply your function and plot your graph with `networkx.draw`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564d8cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "G = get_networkx(mapper_complex, simplex_tree, get_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f7561",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "nx.draw(G, node_size=[G.nodes[k]['size'] for k in G.nodes()], \n",
    "           node_color=[G.nodes[k]['color'] for k in G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a34ac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As seen in class, we can now compute a bag-of-feature descriptor for Mapper, defined as the extended persistence diagram of the Mapper complex associated to the filter. Compute and visuzalize this descriptor with the `compute_PD` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80605902",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgm = mapper_complex.compute_PD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c8aa1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter([p[0] for p in dgm],[p[1] for p in dgm])\n",
    "plt.plot([np.array(dgm).min(), np.array(dgm).max()], [np.array(dgm).min(), np.array(dgm).max()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdda7a6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Can you guess the parts of the 3D shape that are associated to each persistence diagram point?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd3e6f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to understand the parts that are relevant, we can use the (empirical) bootstrap to generate a distribution of bottleneck distances (computed as the distances between our current persistence diagram and a distribution of persistence diagrams obtained from bootstrapped Mapper complexes), and use this distribution to derive confidence regions. Compute first such a distribution with the `compute_distribution` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454234b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mapper_complex.compute_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce431f20",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, fix a confidence threshold, say 90%, and retrieve the bottleneck distance value $d_b^*$ such that 90% of distances are below this value. You can use the `compute_distance_from_confidence_level` function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27b6b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dist_to_diag = mapper_complex.compute_distance_from_confidence_level(.9)\n",
    "print('90% level bottleneck distance = ' + str(dist_to_diag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba670f1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, retrieve the points of our current persistence diagram whose distance to the diagonal is larger than $d_b^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e07ea4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgm_dists_to_diag = np.array([ np.abs(p[1]-p[0])/2 for p in dgm ])\n",
    "robust_points = np.argwhere(dgm_dists_to_diag >= dist_to_diag).ravel()\n",
    "print('Robust point indices are: ' + str(robust_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6b561",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some points were assessed as non robust, can you guess why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83286eb1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, one might ask whether there is a direct map from the points of the persistence diagrams to the parts of the 3D shape. It is actually a non-trivial question for Mappers of dimension greater than 2, but for Mappers in dimension 1, it is easier. Indeed, connected components and loops (corresponding to persistence diagram points in ${\\rm Ext}_0$ and ${\\rm Ext}_1$ respectively---see class) are standard graph features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761bab0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute and visualize the connected components with the `connected_components` function of `networkx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369ef78",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_ccs = nx.connected_components(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd0c602",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for cc in list_ccs:\n",
    "    plt.figure()\n",
    "    nx.draw(G, node_size=[G.nodes[k]['size'] for k in G.nodes()], \n",
    "               node_color=[1 if k in cc else 0 for k in G.nodes()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398909f4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute and visualize the loops with the `cycle_basis` function of `networkx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac85c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_cycles = nx.cycle_basis(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca1609",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for cycle in list_cycles:\n",
    "    plt.figure()\n",
    "    nx.draw(G, node_size=[G.nodes[k]['size'] for k in G.nodes()], \n",
    "               node_color=[1 if k in cycle else 0 for k in G.nodes()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bcabc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, concerning branches, i.e., points in ${\\rm Ord}_0$ and ${\\rm Rel}_1$, the question is a bit more tricky, but fortunately one can use ToMATo as an approximate solution. This is because ToMATo keeps track of the points forming  connected components that are merged later on, wich correspond to branches! Hence, one can apply ToMATo with the filter (resp. the opposite of the filter) to obtain the upward (resp. downward) branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b9437",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since ToMATo requires neighborhood graphs as inputs, write a function `get_neighborhood_graph_from_simplex_tree` that computes the neighborhood graph associated to the 1-skeleton of a simplex tree, in a format that is acceptable for ToMATo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86baf49a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_neighborhood_graph_from_simplex_tree(simplex_tree):\n",
    "    NG = [[] for _ in range(simplex_tree.num_vertices())]\n",
    "    for s,_ in simplex_tree.get_skeleton(1):\n",
    "        if len(s) == 2:\n",
    "            NG[s[0]].append(s[1])\n",
    "            NG[s[1]].append(s[0])\n",
    "    return NG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e6283",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, compute this neighborhood graph and apply ToMATo using both the filter function and its opposite (with no prior on the number of clusters or merging threshold). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98990133",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neighborhood_graph = get_neighborhood_graph_from_simplex_tree(simplex_tree)\n",
    "function = np.array([G.nodes[k[0]]['color'] for k,_ in simplex_tree.get_skeleton(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d85a82",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tomato_1 = gdt.Tomato(graph_type='manual', density_type='manual', n_clusters=None, merge_threshold=-1)\n",
    "tomato_1 = tomato_1.fit(X=neighborhood_graph, weights=function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff55921",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tomato_2 = gdt.Tomato(graph_type='manual', density_type='manual', n_clusters=None, merge_threshold=-1)\n",
    "tomato_2 = tomato_2.fit(X=neighborhood_graph, weights=-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20af400",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, visualize the ToMATo labels on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407d600",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "nx.draw(G, node_size=[G.nodes[k]['size'] for k in G.nodes()], node_color=tomato_1.labels_[G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46572403",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "nx.draw(G, node_size=[G.nodes[k]['size'] for k in G.nodes()], node_color=tomato_2.labels_[G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d76e6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The branches should be detected and highlighted with different colors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb4c71",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section 3: 3D shape statistics with persistence diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c1a2a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, our goal is to compute confidence regions associated to the persistence diagram of a given 3D shape. We will study such regions for both the persistence diagram, and one of its representation, the persistence landscape. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d99781",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's first pick a 3D shape. Let's first pick a 3D shape. For instance, use `Hand/181.off` (or any other one you would like to try)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7f648",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vertices, faces = off2numpy('3dshapes/Vase/361.off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf81d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mp.plot(vertices, faces, c=vertices[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23242c5b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first standard way of obtaining confidence regions for (geometric) persistence diagrams is through the stability theorem (see class):\n",
    "\n",
    "$$\\mathbb{P}(d_b(D_{\\rm Rips}(X),D_{\\rm Rips}(\\hat X_n)) \\geq \\delta)\\leq \\mathbb{P}(d_H(X,\\hat X_n)\\geq \\delta/2),$$\n",
    "$$\\mathbb{P}(d_b(D_{\\rm Cech}(X),D_{\\rm Cech}(\\hat X_n)) \\geq \\delta)\\leq \\mathbb{P}(d_H(X,\\hat X_n)\\geq \\delta),$$\n",
    "\n",
    "where $d_H(\\cdot,\\cdot)$ is the Hausdorff distance, defined, for any two compact spaces $X,Y\\subset \\mathbb{R}^d$, as \n",
    "\n",
    "$$d_H(X,Y)={\\rm min}\\{{\\rm max}_{x\\in X}{\\rm min}_{y\\in Y}\\|x-y\\|, {\\rm max}_{y\\in Y}{\\rm min}_{x\\in X}\\|y-x\\|\\}.$$\n",
    "\n",
    "Hence, it suffices to estimate $\\mathbb{P}(d_H(X,\\hat X_n)\\geq \\delta)$ in order to derive confidence regions for persistence diagrams. There exists an upper bound for this probability when $\\hat X_n$ is drawn from an $(a,b)$-standard probability measure, however this bound depends on $a$ and $b$. In the following, we will rather use the subsampling method, that allows to estimate the probability solely from subsampling $\\hat X_n$ with $s(n) =o\\left(\\frac{n}{{\\rm log}(n)}\\right)$ points, and computing $d_H(\\hat X_n, \\hat X_{s(n)})$. The exact procedure is described in Section 4.1 in [this article](file:///user/mcarrier/home/Downloads/14-AOS1252.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7850260",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write a function `hausdorff_distance` that computes the Hausdorff distance between the vertices of our 3D shape and a subset of these vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643ed89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hausdorff_distance(vertices, sn):\n",
    "    n = len(vertices)\n",
    "    I = np.random.choice(n, sn, replace=False)\n",
    "    Icomp = np.setdiff1d(np.arange(n), I)\n",
    "    tree = skn.KDTree(vertices[I], leaf_size=2)\n",
    "    distances, _ = tree.query(vertices[Icomp], k=1) \n",
    "    hdist = max(distances)\n",
    "    return(hdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6048821",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, write a function `hausdorff_interval` that computes this Hausdorff distance many times and uses the corresponding distribution of Hausdorff distances in order to output the bottleneck distance value associated to a given confidence level (by computing the quantile---corresponding to this confidence level---of the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99374dbe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hausdorff_interval(vertices, level=0.95, sn=100, N=1000):\n",
    "    distribution_hausdorff_distances = [hausdorff_distance(vertices, sn) for _ in range(N)]\n",
    "    diagram_quantile = np.quantile(distribution_hausdorff_distances, level)\n",
    "    return diagram_quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad02abe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Apply your code to obtain a bottleneck distance associated to, say, 90% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f109cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = len(vertices)\n",
    "conf_bottleneck = hausdorff_interval(vertices=vertices, level=0.9, N=100, sn=int(10*n/np.log(n)**2))\n",
    "print('Bottleneck distance associated to confidence level = ' + str(conf_bottleneck))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8009cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All right, now let's see which points of the persistence diagram are we going to label non-significant and discard. Compute the Rips and Alpha persistence diagrams of the points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8fdaf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simplex_tree = gd.RipsComplex(points=vertices, max_edge_length=5*1e-1).create_simplex_tree(max_dimension=2)\n",
    "pers_rips = simplex_tree.persistence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c59c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simplex_tree = gd.AlphaComplex(points=vertices).create_simplex_tree()\n",
    "for splx, filt in simplex_tree.get_filtration():\n",
    "    simplex_tree.assign_filtration(splx, filtration=np.sqrt(filt))\n",
    "pers_alpha = simplex_tree.persistence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba6309",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, visualize the persistence diagrams with a band of size the previously computed bottleneck distance times 2 (for Alpha filtration) and 4 (for Rips filtration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdeef24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gd.plot_persistence_diagram(pers_alpha, band=2*conf_bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993406f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gd.plot_persistence_diagram(pers_rips, band=4*conf_bottleneck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd4d4f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Are you discarding many points? If yes, this could be because the confidence region is computed only from the stability property of persistence diagrams: subsampling the Hausdorff distance can sometimes be quite conservative. It would be more efficient to bootstrap the persistence diagrams themselves---this is the approach advocated in Section 6 of [this article](https://www.jmlr.org/papers/volume18/15-484/15-484.pdf). However, this method was only proved for persistence diagrams obtained through the sublevel sets of kernel density estimators... But let's try it anyway! ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be247ea1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Similarly than before, write `bottleneck_distance_bootstrap` and `bottleneck_interval` functions that compute the bottleneck distances between our current persistence diagram (in homology dimension 1) and the persistence diagrams of many bootstrap iterates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9aafa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgm = simplex_tree.persistence_intervals_in_dimension(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e0a1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bottleneck_distance_bootstrap(dgm, vertices):\n",
    "    n = len(vertices)\n",
    "    I = np.random.choice(n, n, replace=True)\n",
    "    simplex_tree = gd.AlphaComplex(points=vertices[I]).create_simplex_tree()\n",
    "    for splx, filt in simplex_tree.get_filtration():\n",
    "        simplex_tree.assign_filtration(splx, filtration=np.sqrt(filt))\n",
    "    simplex_tree.persistence()\n",
    "    dgm_subsample = simplex_tree.persistence_intervals_in_dimension(1)\n",
    "    return gd.bottleneck_distance(dgm, dgm_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57731a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bottleneck_interval(dgm, vertices, level=0.95, N=1000):\n",
    "    distribution_bottleneck_distances = [bottleneck_distance_bootstrap(dgm, vertices) for _ in range(N)]\n",
    "    bottleneck_diagram_quantile = np.quantile(distribution_bottleneck_distances, level)\n",
    "    return bottleneck_diagram_quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f50b20",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute the bottleneck distance associated to a confidence level and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c430b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = len(vertices)\n",
    "conf_bottleneck_empirical = bottleneck_interval(dgm=dgm, vertices=vertices, level=0.9, N=100)\n",
    "print('Bottleneck distance associated to confidence level = ' + str(conf_bottleneck_empirical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aef2f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gd.plot_persistence_diagram(pers_alpha, band=2*conf_bottleneck_empirical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d3558",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Are you discarding less points in the persistence diagram now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcb3bb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another approach with more theoretical guarantees is to use the persistence landscapes associated to the persistence diagram. Indeed, valid confidence regions can be easily obtained using, e.g., algorithm 1 in [this article](https://geometrica.saclay.inria.fr/team/Fred.Chazal/papers/cflrw-scpls-14/cflrw-scpls-14.pdf). In the following, we will fix a subsample size $s(n)$, and estimate $\\mathbb{E}[\\Lambda_{s(n)}]$, where $\\Lambda_{s(n)}$ is the landscape of a random subsample of size $s(n)$ (i.e., drawn from a probability measure $\\mu$ such as, e.g., the empirical measure). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fabab1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's first make sure that we can compute landscapes ;-) Use `Gudhi` to compute and plot the first six persistence landscapes associated to the persistence diagram computed above in homology dimension 1. Landscapes (and other vectorizations) are implemented with the API of `Scikit-Learn` estimators, which means that you have to call the `fit_transform` method on a list of persistence diagrams in order to get their landscapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd131e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgm1 = simplex_tree.persistence_intervals_in_dimension(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534217fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "landscape1 = gdr.Landscape(num_landscapes=6, resolution=100).fit_transform([dgm1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938cd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(landscape1[0,0:100])\n",
    "plt.plot(landscape1[0,100:200])\n",
    "plt.plot(landscape1[0,200:300])\n",
    "plt.plot(landscape1[0,300:400])\n",
    "plt.plot(landscape1[0,400:500])\n",
    "plt.plot(landscape1[0,500:600])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62a3002",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write a function `landscape_interval` that implements the landscape bootstrap procedure, that is, drawing many subsamples of size $s(n)$, computing their Alpha persistence diagrams and landscapes, computing the distribution of distances between each single landscape and their mean (multiplied by a random normal variable), and finally using the quantiles of this distribution in order to obtain confidence regions for the mean landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfcaad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def landscape_interval(vertices, sn=100, N=100, B=100, num_landscapes=6, resolution=100, landscape_estimator=None):\n",
    "    n = len(vertices)\n",
    "    \n",
    "    list_sub_dgm1 = []\n",
    "    for _ in range(N):\n",
    "        sub_vertices = vertices[np.random.choice(n, sn, replace=True)]\n",
    "        sub_simplex_tree = gd.AlphaComplex(points=sub_vertices).create_simplex_tree()\n",
    "        for splx, filt in sub_simplex_tree.get_filtration():\n",
    "            sub_simplex_tree.assign_filtration(splx, filtration=np.sqrt(filt))\n",
    "        sub_simplex_tree.persistence()\n",
    "        sub_dgm1 = sub_simplex_tree.persistence_intervals_in_dimension(1)\n",
    "        list_sub_dgm1.append(sub_dgm1)\n",
    "    \n",
    "    if landscape_estimator is None:\n",
    "        landscape_estimator = gdr.Landscape(num_landscapes=num_landscapes, resolution=resolution)\n",
    "    \n",
    "    landscape_distrib = landscape_estimator.fit_transform(list_sub_dgm1)\n",
    "    mean_landscape = np.mean(landscape_distrib, axis=0)\n",
    "    landscape_differences = landscape_distrib - mean_landscape[None,:]\n",
    "    \n",
    "    theta_distrib = [[] for _ in range(num_landscapes)]\n",
    "    for _ in range(B):\n",
    "        xi = np.random.normal(size=[N,1])\n",
    "        random_landscape_differences = np.abs(np.multiply(xi, landscape_differences).sum(axis=0))/np.sqrt(N)\n",
    "        for nl in range(num_landscapes):\n",
    "            theta_distrib[nl].append( random_landscape_differences[nl*resolution:(nl+1)*resolution].max() )\n",
    "    \n",
    "    return landscape_estimator, mean_landscape, theta_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e44ae",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Apply and visualize the confidence interval around the different landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab45b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N, B, num_landscapes, resolution = 10, 10, 6, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82897961",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "landscape_estimator, mean_landscape, theta_distrib = landscape_interval(vertices, sn=int(.9*len(vertices)), \n",
    "                                                                        N=N, B=B, num_landscapes=num_landscapes, \n",
    "                                                                        resolution=resolution)\n",
    "q_alpha = [np.quantile(theta_distrib[nl], .9) for nl in range(num_landscapes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e51e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nl = 5\n",
    "\n",
    "mean_curve  = mean_landscape[nl*resolution:(nl+1)*resolution]\n",
    "upper_curve = mean_curve+q_alpha[nl]/np.sqrt(N)\n",
    "lower_curve = np.maximum(0,mean_curve-q_alpha[nl]/np.sqrt(N))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mean_curve)\n",
    "plt.fill_between(np.arange(resolution), lower_curve, upper_curve, alpha=0.2, color='tab:orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d878f7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The confidence regions are much better now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2bcd64",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another interesting property of mean landscapes is their robustness to noise:\n",
    "\n",
    "$$\\|\\mathbb{E}[\\Lambda_{s(n)}^X]-\\mathbb{E}[\\Lambda_{s(n)}^Y]\\|_\\infty\\leq 2 \\cdot s(n) \\cdot d_{GW}(\\mu,\\nu),$$\n",
    "\n",
    "where $d_{GW}$ is the 1-Gromov-Wasserstein distance between probability measures. See Remark 6 in [this article](https://geometrica.saclay.inria.fr/team/Fred.Chazal/papers/cflmrw-smph-15/ICMLFinal.pdf). We will now confirm this by adding outlier noise to the 3D shape and looking at the resulting mean landscape.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da630a5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a noisy version of `vertices` with some outlier noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af712f1a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_noisy_points = 100\n",
    "noisy_vertices = np.vstack([vertices, np.random.uniform(vertices.min(), vertices.max(), [num_noisy_points,3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de280a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's first compare the persistence landscapes of the two sets of vertices. Compute and visualize these landscapes on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be7b1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simplex_tree = gd.AlphaComplex(points=vertices).create_simplex_tree()\n",
    "for splx, filt in simplex_tree.get_filtration():\n",
    "    simplex_tree.assign_filtration(splx, filtration=np.sqrt(filt))\n",
    "simplex_tree.persistence()\n",
    "dgm1 = simplex_tree.persistence_intervals_in_dimension(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fdece",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "noisy_simplex_tree = gd.AlphaComplex(points=noisy_vertices).create_simplex_tree()\n",
    "for splx, filt in noisy_simplex_tree.get_filtration():\n",
    "    noisy_simplex_tree.assign_filtration(splx, filtration=np.sqrt(filt))\n",
    "noisy_simplex_tree.persistence()\n",
    "noisy_dgm1 = noisy_simplex_tree.persistence_intervals_in_dimension(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e72aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "landscape1 = gdr.Landscape(num_landscapes=6, resolution=100).fit_transform([dgm1, noisy_dgm1])\n",
    "plt.figure()\n",
    "plt.plot(landscape1[0,0:100])\n",
    "plt.plot(landscape1[0,100:200])\n",
    "plt.plot(landscape1[0,200:300])\n",
    "plt.plot(landscape1[0,300:400])\n",
    "plt.plot(landscape1[0,400:500])\n",
    "plt.plot(landscape1[0,500:600])\n",
    "\n",
    "plt.plot(landscape1[1,0:100],   linestyle='--')\n",
    "plt.plot(landscape1[1,100:200], linestyle='--')\n",
    "plt.plot(landscape1[1,200:300], linestyle='--')\n",
    "plt.plot(landscape1[1,300:400], linestyle='--')\n",
    "plt.plot(landscape1[1,400:500], linestyle='--')\n",
    "plt.plot(landscape1[1,500:600], linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a205f6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As one can see, they are quite different. By contrast, computing the mean landscape with subsamples is much more robust, as we will now see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608815aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute the mean persistence landscape of the noisy point cloud, and visualize it next to the mean persistence landscape of the clean point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fab550",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_, noisy_mean_landscape, noisy_theta_distrib = landscape_interval(noisy_vertices, sn=int(.9*len(vertices)), \n",
    "                                                               N=N, B=B, num_landscapes=num_landscapes, \n",
    "                                                               resolution=resolution,\n",
    "                                                               landscape_estimator=landscape_estimator)\n",
    "noisy_q_alpha = [np.quantile(noisy_theta_distrib[nl], .9) for nl in range(num_landscapes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655457a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nl = 5\n",
    "\n",
    "mean_curve  = mean_landscape[nl*resolution:(nl+1)*resolution]\n",
    "upper_curve = mean_curve+q_alpha[nl]/np.sqrt(N)\n",
    "lower_curve = np.maximum(0,mean_curve-q_alpha[nl]/np.sqrt(N))\n",
    "\n",
    "noisy_mean_curve  = noisy_mean_landscape[nl*resolution:(nl+1)*resolution]\n",
    "noisy_upper_curve = noisy_mean_curve+noisy_q_alpha[nl]/np.sqrt(N)\n",
    "noisy_lower_curve = np.maximum(0,noisy_mean_curve-noisy_q_alpha[nl]/np.sqrt(N))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mean_curve, c='red')\n",
    "plt.fill_between(np.arange(resolution), lower_curve, upper_curve, alpha=0.2, color='tab:orange')\n",
    "plt.plot(noisy_mean_curve, c='black')\n",
    "plt.fill_between(np.arange(resolution), noisy_lower_curve, noisy_upper_curve, alpha=0.2, color='tab:blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55589f1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, these landscapes looks much more in agreement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156df58e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Section 4: 3D shape classification with persistence diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d607a8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section, our goal is to use persistence diagrams for classifying and segmenting 3D shapes with supervised machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809a0eb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's start with classification. We will compute persistence diagrams for all shapes in different categories, and train a classifier from `Scikit-Learn` to predict the category from the persistence diagrams. Since `Gudhi` requires simplex trees from the persistence diagram computations, write a `get_simplex_tree_from_faces` function that builds a simplex tree from the faces of a given 3D shape triangulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd503d14",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_simplex_tree_from_faces(faces):\n",
    "    simplex_tree = gd.SimplexTree()\n",
    "    for face in faces:\n",
    "        simplex_tree.insert(face, -1e10)\n",
    "    return simplex_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca315b3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, compute all the persistence diagrams (in homology dimension 0) associated to the sublevel sets of the third coordinate from a few categories, and retrieve their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4bc48",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_categories = os.listdir(dataset_path)\n",
    "print(all_categories)\n",
    "categories_to_classify = [all_categories[1], all_categories[2], all_categories[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af503167",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgms, labels = [], []\n",
    "for label in all_categories[:3]:\n",
    "    shapes = os.listdir(dataset_path + label + '/')\n",
    "    for file in shapes:\n",
    "        if file[-4:] == '.off':\n",
    "            vertices, faces = off2numpy(dataset_path + label + '/' + file)\n",
    "            st = get_simplex_tree_from_faces(faces)\n",
    "            filtration = vertices[:,2]\n",
    "            for v in range(len(vertices)):\n",
    "                st.assign_filtration([v], filtration[v])\n",
    "            st.make_filtration_non_decreasing()\n",
    "            st.persistence()\n",
    "            dgms.append(st.persistence_intervals_in_dimension(0)) \n",
    "            labels.append(label)\n",
    "le = skp.LabelEncoder().fit(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c43c9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As discussed in class, it is not very convenient to use persistence diagrams directly for machine learning purposes (except for a few methods such as $K$-nearest neighbors). What we need is to define a vectorization, that is, a map $\\Phi:\\mathcal{D}\\rightarrow\\mathcal{H}$ sending persistence diagrams into a Hilbert space, or equivalently,  a symmetric kernel function $k:\\mathcal{D}\\times \\mathcal{D} \\rightarrow \\mathbb{R}$ such that $k(D,D')=\\langle \\Phi(D),\\Phi(D')\\rangle$. Fortunately, there are already a bunch of such maps and kernels in `Gudhi` :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01453c8e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the following we will compute and visualize the most popular kernels on some persistence diagrams. Pick first a specific persistence diagram and use `DiagramSelector` to remove its points with infinite coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a560",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diagram = dgms[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee856bff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gd.plot_persistence_diagram(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85722d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[diagram] = gdr.DiagramSelector(use=True, point_type='finite').fit_transform([diagram])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35424c2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, let's see what `Gudhi` has to offer to vectorize persistence diagrams with `Scikit-Learn` estimator-like classes, that is, with classes that have `fit`, `transform`, and `fit_transform` methods, see [this article](https://arxiv.org/pdf/1309.0238.pdf) for more details. For each vectorization mentioned below, we recommend you to play with its parameters and infer their influence on the ouput in order to get some intuition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea5bc0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first vectorization method that was introduced historically is the persistence landscape. A persistence landscape is basically obtained by rotating the persistence diagram by $-\\pi/4$\n",
    "(so that the diagonal becomes the $x$-axis), and then putting tent functions on each point. The $k$th landscape is then defined as the $k$th largest value among all these tent functions. It is eventually turned into a vector by evaluating it on a bunch of uniformly sampled points on the $x$-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c60599",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute and visualize the first landscape of the persistence diagram for various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da91b2d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LS = gdr.Landscape(resolution=1000, num_landscapes=3)\n",
    "L = LS.fit_transform([diagram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b9123",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(L[0][:1000])\n",
    "plt.plot(L[0][1000:2000])\n",
    "plt.plot(L[0][2000:3000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853ba6a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A variation, called the silhouette, takes a weighted average of these tent functions instead. Here, we weight each tent function by the distance of the corresponding point to the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827f28a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SH = gdr.Silhouette(resolution=1000, weight=lambda x: np.power(x[1]-x[0],2))\n",
    "sh = SH.fit_transform([diagram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65a76e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sh[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a71ac7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The second method is the persistence image. A persistence image is obtained by rotating by $-\\pi/4$, centering Gaussian functions on all diagram points (usually weighted by a parameter function, such as, e.g., the squared distance to the diagonal) and summing all these Gaussians. This gives a 2D function, that is pixelized into an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da23ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PI = gdr.PersistenceImage(bandwidth=5*1e-2, weight=lambda x: x[1]**0, \\\n",
    "                          im_range=[-.5,.5,0,.5], resolution=[100,100])\n",
    "pi = PI.fit_transform([diagram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752572af",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.flip(np.reshape(pi[0], [100,100]), 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bf059",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Gudhi` also has a variety of metrics and kernels, which sometimes perform better than explicit vectorizations such as the ones described above. Pick another persistence diagram, and get familiar with the bottleneck and the Wasserstein distances between them. Note that you can call them in different ways in `Gudhi`, there are `bottleneck_distance` and `wasserstein_distance` functions for instance, but there are also wrappers of these functions into estimator classes `BottleneckDistance` and `WassersteinDistance` (with `fit` and `transform` methods). These classes are especially useful when doing model selection with `Scikit-Learn`, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8466fca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[diagram_bis] = gdr.DiagramSelector(use=True, point_type='finite').fit_transform([dgms[20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28efc8e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gd.plot_persistence_diagram(diagram_bis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b6074",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BD = gdr.BottleneckDistance(epsilon=.001)\n",
    "BD.fit([diagram])\n",
    "bd = BD.transform([diagram_bis])\n",
    "print(\"Bottleneck distance is \" + str(bd[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b6863",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "WD = gdr.WassersteinDistance(internal_p=2, order=2)\n",
    "WD.fit([diagram])\n",
    "wd = WD.transform([diagram_bis])\n",
    "print(\"Wasserstein distance is \" + str(wd[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b1745",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Gudhi` also allows to use standard kernels such as, among others, the persistence scale space kernel, persistence Fisher kernel, sliced Wasserstein kernel, etc. Try computing the kernel values for your pair of diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b7a53",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PSS = gd.representations.PersistenceScaleSpaceKernel(bandwidth=1.)\n",
    "PSS.fit([diagram])\n",
    "pss = PSS.transform([diagram_bis])\n",
    "print(\"PSS kernel is \" + str(pss[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92371813",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PF = gd.representations.PersistenceFisherKernel(bandwidth_fisher=.1, bandwidth=.1, kernel_approx=None)\n",
    "PF.fit([diagram])\n",
    "pf = PF.transform([diagram_bis])\n",
    "print(\"PF kernel is \" + str(pf[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d53c1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SW = gd.representations.SlicedWassersteinKernel(bandwidth=1, num_directions=100)\n",
    "SW.fit([diagram])\n",
    "sw = SW.transform([diagram_bis])\n",
    "print(\"SW kernel is \" + str(sw[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b147f7c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before trying to classify the persistence diagrams, let's do a quick dimension reduction with PCA. Apply `PCA`, `KernelPCA` or `MDS` (available in `Scikit-Learn`) on the explicit maps (landscapes, images, etc), kernel matrices (Fisher, sliced Wasserstein, etc) and distance matrices (bottleneck, Wasserstein, etc) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc92ffd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgms = gdr.DiagramSelector(use=True, point_type='finite').fit_transform(dgms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e9612",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "silhouettes        = gdr.Silhouette(resolution=1000, weight=lambda x: np.power(x[1]-x[0],0)).fit_transform(dgms)\n",
    "sliced_wass_kernel = gdr.SlicedWassersteinKernel(num_directions=100).fit_transform(dgms)\n",
    "bottleneck_matrix  = gdr.BottleneckDistance().fit_transform(dgms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478a02c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca_silhouettes         = skd.PCA(n_components=2).fit_transform(silhouettes)\n",
    "kpca_sliced_wass_kernel = skd.KernelPCA(n_components=2).fit_transform(sliced_wass_kernel)\n",
    "mds_bottleneck_matrix   = skf.MDS(n_components=2, dissimilarity='precomputed').fit_transform(bottleneck_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc92426",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "integer_labels = le.transform(labels)\n",
    "label_indices = [(l,le.classes_[l],np.argwhere(integer_labels==l).ravel()) for l in range(integer_labels.max()+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c70b0e",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax1 = plt.subplot(131)\n",
    "for il,l,li in label_indices:\n",
    "    ax1.scatter(pca_silhouettes[li,0], pca_silhouettes[li,1], label=l)\n",
    "ax1.legend()\n",
    "ax1.set_title('Silhouette')\n",
    "ax2 = plt.subplot(132)\n",
    "for il,l,li in label_indices:\n",
    "    ax2.scatter(kpca_sliced_wass_kernel[li,0], kpca_sliced_wass_kernel[li,1], label=l)\n",
    "ax2.legend()\n",
    "ax2.set_title('SWK')\n",
    "ax3 = plt.subplot(133)\n",
    "for il,l,li in label_indices:\n",
    "    ax3.scatter(mds_bottleneck_matrix[li,0], mds_bottleneck_matrix[li,1], label=l)\n",
    "ax3.legend()\n",
    "ax3.set_title('Bottleneck')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97691c7b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Is there any method that looks better in separating the categories, at least by eye?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9358a76",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All right, let's try classification now! Shuffle the data, and create a random 80/20 train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f7cf4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "test_size            = 0.2\n",
    "perm                 = np.random.permutation(len(labels))\n",
    "limit                = int(test_size * len(labels))\n",
    "test_sub, train_sub  = perm[:limit], perm[limit:]\n",
    "train_labs           = np.array(labels)[train_sub]\n",
    "test_labs            = np.array(labels)[test_sub]\n",
    "train_dgms           = [dgms[i] for i in train_sub]\n",
    "test_dgms            = [dgms[i] for i in test_sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c888ba",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is the best thing about having estimator-like classes: they can be integrated flawlessly in a `Pipeline` of `Scikit-Learn` for model selection and cross-validation! A `Pipeline` is itself an estimator, and is initialized as with a list of estimators. It will just sequentially apply the `fit_transform` methods of the estimators in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852854a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define a `Pipeline` with four estimators: one for selecting the finite persistence diagram points, one for scaling (or not) the persistence diagrams (with `DiagramScaler`), one for vectorizing persistence diagrams, and one for performing the final prediction. See the [documentation](https://scikit-learn.org/stable/modules/compose.html#combining-estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8427f7d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipe = skl.Pipeline([(\"Separator\", gdr.DiagramSelector(limit=np.inf, point_type=\"finite\")),\n",
    "                     (\"Scaler\",    gdr.DiagramScaler(scalers=[([0,1], skp.MinMaxScaler())])),\n",
    "                     (\"TDA\",       gdr.PersistenceImage()),\n",
    "                     (\"Estimator\", sks.SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e929b7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, define a grid of parameter that will be used in cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e27850",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param =    [{\"Separator__use\":      [True],\n",
    "             \"Scaler__use\":         [False, True],\n",
    "             \"TDA\":                 [gdr.SlicedWassersteinKernel()], \n",
    "             \"TDA__bandwidth\":      [0.1, 1.0],\n",
    "             \"TDA__num_directions\": [20],\n",
    "             \"Estimator\":           [sks.SVC(kernel=\"precomputed\", gamma=\"auto\")]},\n",
    "                        \n",
    "            {\"Separator__use\":      [True],\n",
    "             \"Scaler__use\":         [False, True],\n",
    "             \"TDA\":                 [gdr.Silhouette()], \n",
    "             \"TDA__resolution\":     [100],\n",
    "             \"Estimator\":           [ske.RandomForestClassifier()]},\n",
    "           \n",
    "            {\"Separator__use\":      [True],\n",
    "             \"Scaler__use\":         [False, True],\n",
    "             \"TDA\":                 [gdr.BottleneckDistance()], \n",
    "             \"TDA__epsilon\":        [0.1], \n",
    "             \"Estimator\":           [skn.KNeighborsClassifier(metric=\"precomputed\")]}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be07934",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474d0df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = skm.GridSearchCV(pipe, param, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b3490",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = model.fit(train_dgms, train_labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2c18d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check the parameters that were chosen during model selection, and evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821d29f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad44c2d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(model.score(train_dgms, train_labs)))\n",
    "print(\"Test accuracy  = \" + str(model.score(test_dgms,  test_labs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23016f4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How is your score? If it is bad, you can always increase the parameter and/or classifier search, but this can quickly become quite cumbersome. Moreover, a potential source of error comes from the fact that the third coordinate do not necessarily correspond to the height (i.e., the 3D shapes are not embedded in a consistent way). This is where persistence differentiation can come to the rescue! Indeed, instead of picking a specific coordinate, we can try to optimize a linear combination of the coordinates:\n",
    "\n",
    "$$f_\\alpha: x\\mapsto \\sum_{i=1}^d \\alpha_i x_i,$$\n",
    "\n",
    "such that the persistence diagrams of the same category are close, while persistence diagrams from different categories are far away from each other. This means minimizing a loss of the form:\n",
    "\n",
    "$$\\alpha^* = {\\rm min}_\\alpha \\sum_l \\frac{\\sum_{y_i=y_j=l}d(D_{f_\\alpha}(x_i),D_{f_\\alpha}(x_j))}{\\sum_{y_i=l,y_j}d(D_{f_\\alpha}(x_i),D_{f_\\alpha}(x_j))},$$\n",
    "\n",
    "where $d$ is any (pseudo)-distance between persistence diagrams, that can be differentiated through a deep learning library (such as `TensorFlow` or `PyTorch`). For instance, the sliced Wasserstein distance is quite easy to compute with standard deep learning libraries since it only involves projecting points onto lines. See [this article](http://proceedings.mlr.press/v70/carriere17a/carriere17a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0093acc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write a `deep_swd` function that computes the sliced Wasserstein distance between persistence diagrams with `TensorFlow` or `PyTorch` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4de040",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def deep_swd(dgms, ccards, thetas):\n",
    "    projected_dgms = tf.linalg.matmul(tf.concat(dgms,axis=0), .5*tf.ones([2,2], tf.float32))\n",
    "    dgms_big = tf.concat([tf.reshape(\n",
    "        tf.concat([dgm, projected_dgms[:ccards[idg]], projected_dgms[ccards[idg+1]:]], axis=0), \n",
    "        [-1,2,1,1]) for idg, dgm in enumerate(dgms)], axis=2)\n",
    "    cosines, sines = tf.math.cos(thetas), tf.math.sin(thetas)\n",
    "    vecs = tf.concat([tf.reshape(cosines,[1,1,1,-1]), tf.reshape(sines,[1,1,1,-1])], axis=1)\n",
    "    theta_projs = tf.sort(tf.math.reduce_sum(tf.math.multiply(dgms_big, vecs), axis=1), axis=0)\n",
    "    t1 = tf.reshape(theta_projs, [ccards[-1],-1,1,100])\n",
    "    t2 = tf.reshape(theta_projs, [ccards[-1],1,-1,100])\n",
    "    dists = tf.math.reduce_mean(tf.math.reduce_sum(tf.math.abs(t1-t2), axis=0), axis=2)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174ba5f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, just as before, split the data into train/test, but this time, collect the vertices, simplex trees and labels (it is useless to compute persistence diagrams since they will be recomputed after each gradient descent iteration and update of $\\alpha$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72d2fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shape_vertices, simplex_trees, labels = [], [], []\n",
    "for label in all_categories[:3]:\n",
    "    shapes = os.listdir(dataset_path + label + '/')\n",
    "    for file in shapes:\n",
    "        if file[-4:] == '.off':\n",
    "            vertices, faces = off2numpy(dataset_path + label + '/' + file)\n",
    "            st = get_simplex_tree_from_faces(faces)\n",
    "            shape_vertices.append(vertices)\n",
    "            simplex_trees.append(st)\n",
    "            labels.append(label)\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66b8f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_size            = 0.2\n",
    "perm                 = np.random.permutation(len(labels))\n",
    "limit                = int(test_size * len(labels))\n",
    "test_sub, train_sub  = perm[:limit], perm[limit:]\n",
    "train_labs           = np.array(labels)[train_sub]\n",
    "test_labs            = np.array(labels)[test_sub]\n",
    "train_sts            = [simplex_trees[i] for i in train_sub]\n",
    "test_sts             = [simplex_trees[i] for i in test_sub]\n",
    "train_3ds            = [shape_vertices[i] for i in train_sub]\n",
    "test_3ds             = [shape_vertices[i] for i in test_sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc426cc3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Initialize the alpha values, as well as the angles used for computing the sliced Wasserstein distances (and make sure these angles are not optimized during training). Define also the iteration number, batch size, learning rate and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63df3f48",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alphas = tf.Variable(initial_value=np.array([0,0,1], dtype=np.float32), trainable=True)\n",
    "thetainit = np.linspace(-np.pi/2, np.pi/2, num=100)\n",
    "thetas = tf.Variable(initial_value=np.array(thetainit, dtype=np.float32), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66aee1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.1, decay_steps=1e5, decay_rate=0.99, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98a2cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Run gradient descent! For this, you can use the `LowerStarSimplexTreeLayer` class from `Gudhi`, which computes persistence diagrams from simplex trees in a differentiable way with `TensorFlow` operations. Make sure to save the loss value at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc193bb3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs+1):\n",
    "    \n",
    "    print(str(epoch) + '/' + str(num_epochs))\n",
    "    np.random.seed(int(1e2*epoch))\n",
    "    \n",
    "    npts, total_n, batch_i = 0, len(train_labs), 0\n",
    "    perm = np.random.permutation(total_n)\n",
    "    Lgrads = []\n",
    "\n",
    "    while npts < total_n:\n",
    "\n",
    "        batch = perm[batch_i*batch_size:(batch_i+1)*batch_size]\n",
    "        npts += batch_size\n",
    "        batch_i += 1\n",
    "        weight = len(batch)/total_n\n",
    "        batch_labs = [train_labs[i] for i in batch]\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            dgms, cards = [], [0]\n",
    "            for i in batch:\n",
    "                filtration = tf.math.reduce_sum(tf.math.multiply(alphas, train_3ds[i]), axis=1)\n",
    "                sl = gdtf.LowerStarSimplexTreeLayer(simplextree=train_sts[i], homology_dimensions=[0])\n",
    "                dgm = sl.call(filtration)[0][0]\n",
    "                dgms.append(dgm)\n",
    "                cards.append(dgm.shape[0])\n",
    "        \n",
    "            ccards = np.cumsum(cards)\n",
    "            dists = deep_swd(dgms, ccards, thetas)\n",
    "\n",
    "            loss = 0.\n",
    "            classes = np.unique(batch_labs)\n",
    "            for l in classes:\n",
    "                lidxs = np.argwhere(np.array(batch_labs) == l).ravel()\n",
    "                idxs1 = list(itertools.product(lidxs, lidxs))\n",
    "                idxs2 = list(itertools.product(lidxs, range(len(batch))))\n",
    "                cost1 = tf.math.reduce_sum(tf.gather_nd(dists, idxs1))\n",
    "                cost2 = tf.math.reduce_sum(tf.gather_nd(dists, idxs2))\n",
    "                if cost2 > 0:\n",
    "                    loss += cost1 / cost2\n",
    "                else:\n",
    "                    loss += cost1\n",
    "\n",
    "        \n",
    "        gradients = tape.gradient(loss, [alphas])     \n",
    "        Lgrads.append(weight*gradients[0])\n",
    "\n",
    "    final_grad = gradients\n",
    "    final_grad[0] = tf.math.add_n(Lgrads)\n",
    "    optimizer.apply_gradients(zip(final_grad, [alphas]))\n",
    "\n",
    "    losses.append(loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97cec0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualize the losses. Is it decreasing? What are the final alpha values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026883f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0ca04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_alphas = alphas.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691ec47",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(final_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97ac83",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can now use these values to train a model again with this new filtration, and check whether the accuracy is better now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cd150",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgms, labels = [], []\n",
    "for label in all_categories[:3]:\n",
    "    shapes = os.listdir(dataset_path + label + '/')\n",
    "    for file in shapes:\n",
    "        if file[-4:] == '.off':\n",
    "            vertices, faces = off2numpy(dataset_path + label + '/' + file)\n",
    "            st = get_simplex_tree_from_faces(faces)\n",
    "            filtration = np.multiply(final_alphas[None,:], vertices).sum(axis=1)\n",
    "            for v in range(len(vertices)):\n",
    "                st.assign_filtration([v], filtration[v])\n",
    "            st.make_filtration_non_decreasing()\n",
    "            st.persistence()\n",
    "            dgms.append(st.persistence_intervals_in_dimension(0)) \n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24a9a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_size            = 0.2\n",
    "perm                 = np.random.permutation(len(labels))\n",
    "limit                = int(test_size * len(labels))\n",
    "test_sub, train_sub  = perm[:limit], perm[limit:]\n",
    "train_labs           = np.array(labels)[train_sub]\n",
    "test_labs            = np.array(labels)[test_sub]\n",
    "train_dgms           = [dgms[i] for i in train_sub]\n",
    "test_dgms            = [dgms[i] for i in test_sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3629e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = skm.GridSearchCV(pipe, param, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9c2ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = model.fit(train_dgms, train_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244dcb8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3b619",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Train accuracy = \" + str(model.score(train_dgms, train_labs)))\n",
    "print(\"Test accuracy  = \" + str(model.score(test_dgms,  test_labs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da0ee3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yay! That's definitely better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58d121",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you managed to go that far, congrats, you are basically a TDA expert now ;-) Do not hesitate to reuse these pieces of code for your own research, and let us know if you have any comment/question/suggestion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5c05b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
